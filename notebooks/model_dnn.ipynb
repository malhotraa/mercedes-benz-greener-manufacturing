{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import h5py\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback as cbs\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "import time\n",
    "import csv\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RAW_DATA_PATH = '/kaggle/dev/mercedes-benz-greener-manufacturing-data/raw_data'\n",
    "DATA_PATH = '/kaggle/dev/mercedes-benz-greener-manufacturing-data'\n",
    "TRAIN_DATA = os.path.join(RAW_DATA_PATH, 'train.csv')\n",
    "TEST_DATA = os.path.join(RAW_DATA_PATH, 'test.csv')\n",
    "SAMPLE_SUBMISSION = os.path.join(RAW_DATA_PATH, 'sample_submission.csv')\n",
    "SUBMISSION_PATH = os.path.join(DATA_PATH, 'submissions')\n",
    "MODELS_PATH_NN = os.path.join(DATA_PATH, 'models/nn/')\n",
    "ENSEMBLE_PATH = os.path.join(DATA_PATH, 'ensemble/malhotra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_DATA)\n",
    "test_df = pd.read_csv(TEST_DATA)\n",
    "sample_submission_df = pd.read_csv(SAMPLE_SUBMISSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering outliers\n",
      "train_df.shape (4208, 366)\n",
      "test_df.shape (4209, 365)\n"
     ]
    }
   ],
   "source": [
    "filter_outliers = True\n",
    "filter_XO_X8 = False\n",
    "dnn_use_augment_features = True\n",
    "\n",
    "# Preprocess data\n",
    "for column in train_df.columns:\n",
    "    cardinality = len(np.unique(train_df[column]))\n",
    "    if cardinality == 1:\n",
    "        train_df.drop(column, axis=1, inplace=True)\n",
    "        test_df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "x0_x8 = [\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]\n",
    "original_train_columns = sorted(list(set(train_df.columns) - set(['ID', 'y'])))\n",
    "\n",
    "for f in [\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]:\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    train_values = set(train_df[f].values)\n",
    "    test_values = set(test_df[f].values)\n",
    "    all_values = list(train_values | test_values)\n",
    "    lbl.fit(all_values) \n",
    "    train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "    test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "\n",
    "if filter_outliers:\n",
    "    print('Filtering outliers')\n",
    "    # Filter out outlier y = 265.32\n",
    "    train_df = train_df[train_df.y < 200]\n",
    "    \n",
    "if filter_XO_X8:\n",
    "    print('Filtering XO-X8')\n",
    "    train_df = train_df[list(set(train_df.columns) - set(x0_x8))]\n",
    "    test_df = test_df[list(set(test_df.columns) - set(x0_x8))]\n",
    "    \n",
    "print('train_df.shape', train_df.shape)\n",
    "print('test_df.shape', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original columns 364\n",
      "augmented columns 404\n",
      "train_df.shape (4208, 406)\n",
      "test_df.shape (4209, 405)\n"
     ]
    }
   ],
   "source": [
    "n_comp = 10\n",
    "\n",
    "# tSVD\n",
    "# tsvd = TruncatedSVD(n_components=n_comp, random_state=420)\n",
    "# tsvd_results_train = tsvd.fit_transform(train_df.drop([\"y\"], axis=1))\n",
    "# tsvd_results_test = tsvd.transform(test_df)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=420)\n",
    "pca2_results_train = pca.fit_transform(train_df.drop([\"y\"], axis=1))\n",
    "pca2_results_test = pca.transform(test_df)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=420)\n",
    "ica2_results_train = ica.fit_transform(train_df.drop([\"y\"], axis=1))\n",
    "ica2_results_test = ica.transform(test_df)\n",
    "\n",
    "# GRP\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n",
    "grp_results_train = grp.fit_transform(train_df.drop([\"y\"], axis=1))\n",
    "grp_results_test = grp.transform(test_df)\n",
    "\n",
    "# SRP\n",
    "srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n",
    "srp_results_train = srp.fit_transform(train_df.drop([\"y\"], axis=1))\n",
    "srp_results_test = srp.transform(test_df)\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(0, n_comp):\n",
    "    train_df['pca_' + str(i)] = pca2_results_train[:, i]\n",
    "    test_df['pca_' + str(i)] = pca2_results_test[:, i]\n",
    "\n",
    "    train_df['ica_' + str(i)] = ica2_results_train[:, i]\n",
    "    test_df['ica_' + str(i)] = ica2_results_test[:, i]\n",
    "\n",
    "#     train_df['tsvd_' + str(i)] = tsvd_results_train[:, i]\n",
    "#     test_df['tsvd_' + str(i)] = tsvd_results_test[:, i]\n",
    "\n",
    "    train_df['grp_' + str(i)] = grp_results_train[:, i]\n",
    "    test_df['grp_' + str(i)] = grp_results_test[:, i]\n",
    "\n",
    "    train_df['srp_' + str(i)] = srp_results_train[:, i]\n",
    "    test_df['srp_' + str(i)] = srp_results_test[:, i]\n",
    "\n",
    "augmented_train_columns = sorted(list(set(train_df.columns) - set(['ID', 'y'])))\n",
    "print('original columns', len(original_train_columns))\n",
    "print('augmented columns', len(augmented_train_columns))\n",
    "print('train_df.shape', train_df.shape)\n",
    "print('test_df.shape', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(DATA_PATH, 'train_df_encoded.csv'))\n",
    "test_df.to_csv(os.path.join(DATA_PATH, 'test_df_encoded.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (4208, 404)\n",
      "Y.shape (4208,)\n"
     ]
    }
   ],
   "source": [
    "Y = train_df['y'].values\n",
    "X = train_df.drop([\"y\"], axis=1).set_index(['ID'], drop=True)\n",
    "test_X = test_df.set_index(['ID'], drop=True)\n",
    "print('X.shape', X.shape)\n",
    "print('Y.shape', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv(os.path.join(DATA_PATH, 'X.csv'))\n",
    "test_X.to_csv(os.path.join(DATA_PATH, 'test_X.csv'))\n",
    "np.savetxt(os.path.join(DATA_PATH, 'Y.csv'), Y, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Training 1498536945\n",
      "inputs.shape (?, 404)\n",
      "outputs.shape (?, 1)\n",
      "Fold 1 Training 1498539281\n",
      "inputs.shape (?, 404)\n",
      "outputs.shape (?, 1)\n",
      "Fold 2 Training 1498541615\n",
      "inputs.shape (?, 404)\n",
      "outputs.shape (?, 1)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_FOLDS = 5\n",
    "kf = model_selection.KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "perf = []\n",
    "\n",
    "def r2_keras(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (1 - SS_res / (SS_tot + K.epsilon()))\n",
    "\n",
    "class LossHistory(cbs):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "for fold, (train_idxs, val_idxs) in enumerate(kf.split(X)):\n",
    "    timestamp = str(int(time.time()))\n",
    "    os.mkdir(MODELS_PATH_NN + timestamp)\n",
    "    print('Fold {} Training {}'.format(fold, timestamp))\n",
    "    \n",
    "    np.savetxt(os.path.join(MODELS_PATH_NN + timestamp, 'fold_{}_train_idxs.csv'.format(fold)), train_idxs, delimiter=',')\n",
    "    np.savetxt(os.path.join(MODELS_PATH_NN + timestamp, 'fold_{}_val_idxs.csv'.format(fold)), val_idxs, delimiter=',')\n",
    "\n",
    "    \n",
    "    trn_X, val_X = X.iloc[train_idxs], X.iloc[val_idxs]\n",
    "    trn_Y, val_Y = Y[train_idxs], Y[val_idxs]\n",
    "    \n",
    "    callbacks = [\n",
    "        LossHistory(),\n",
    "    #     EarlyStopping(\n",
    "    #         monitor='val_r2_keras',\n",
    "    #         min_delta=0.001\n",
    "    #         patience=20,\n",
    "    #         mode='max',\n",
    "    #         verbose=0),\n",
    "        ModelCheckpoint(\n",
    "            MODELS_PATH_NN + timestamp + '/' + '{epoch:04d}-{r2_keras:.6f}-{val_r2_keras:.6f}.hdf5', \n",
    "            monitor='r2_keras', \n",
    "            save_best_only=True, \n",
    "            mode='max',\n",
    "            verbose=0)\n",
    "    ]\n",
    "\n",
    "    input_dims = train_X.shape[1]\n",
    "\n",
    "    # This returns a tensor\n",
    "    inputs = Input(shape=(input_dims,))\n",
    "\n",
    "    # a layer instance is callable on a tensor, and returns a tensor\n",
    "\n",
    "    print('inputs.shape', inputs.shape)\n",
    "\n",
    "    x = Dense(input_dims, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Activation('relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(input_dims, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Activation('relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(input_dims, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Activation('relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(input_dims, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Activation('relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(input_dims//2, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Activation('relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Output Layer.\n",
    "    outputs = Dense(1)(x)\n",
    "\n",
    "    print('outputs.shape', outputs.shape)\n",
    "\n",
    "    adamOptimizer = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=adamOptimizer,\n",
    "                  loss='mean_squared_error', #'mean_squared_logarithmic_error',\n",
    "                  metrics=[r2_keras, 'mae', 'mean_squared_logarithmic_error'])\n",
    "\n",
    "    k_X, k_Y = trn_X.copy().values, trn_Y\n",
    "    k_val_X = val_X.copy().values\n",
    "\n",
    "    history = model.fit([k_X], [k_Y], epochs=5000, batch_size=BATCH_SIZE, verbose=0, validation_data=(k_val_X, val_Y), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_keras(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (1 - SS_res / (SS_tot + K.epsilon()))\n",
    "\n",
    "model_id1 = '1498536945/2114-0.478986-0.537906.hdf5'\n",
    "model_id2 = '1498539281/4127-0.507133-0.542570.hdf5'\n",
    "model_id3 = '1498541615/4886-0.502786-0.596412.hdf5'\n",
    "model_id4 = '1498543945/4910-0.501763-0.604505.hdf5'\n",
    "model_id5 = '1498546273/4721-0.513207-0.574646.hdf5'\n",
    "\n",
    "dataset_blend_train = np.zeros((X.shape[0]))\n",
    "dataset_blend_test = np.zeros((test_X.shape[0]))\n",
    "dataset_blend_test_j = np.zeros((test_X.shape[0], NUM_FOLDS))\n",
    "\n",
    "X = pd.read_csv(os.path.join(DATA_PATH, 'X.csv')).set_index(['ID'], drop=True)\n",
    "test_X = pd.read_csv(os.path.join(DATA_PATH, 'test_X.csv')).set_index(['ID'], drop=True)\n",
    "Y = np.loadtxt(os.path.join(DATA_PATH, 'Y.csv'), delimiter=',')\n",
    "\n",
    "k_test_X = test_X.copy().values\n",
    "for fold, model_id in enumerate([model_id1, model_id2, model_id3, model_id4, model_id5]):\n",
    "    model = load_model(MODELS_PATH_NN + model_id, custom_objects={'r2_keras': r2_keras})\n",
    "    val_idxs_path = model_id.split('/')[0] + '/' + 'fold_{}_val_idxs.csv'.format(fold)\n",
    "    val_idxs = np.loadtxt(MODELS_PATH_NN + val_idxs_path, dtype=np.int32)\n",
    "    \n",
    "    val_X = X.iloc[val_idxs]\n",
    "    k_val_X = val_X.copy().values\n",
    "    \n",
    "    test_Y = model.predict(k_test_X).ravel()\n",
    "    val_Y_pred = model.predict(k_val_X).ravel()\n",
    "    \n",
    "    dataset_blend_train[val_idxs] = val_Y_pred\n",
    "    dataset_blend_test_j[:, fold] = test_Y\n",
    "\n",
    "dataset_blend_test = dataset_blend_test_j.mean(1)\n",
    "\n",
    "val_r2_mean = '56370'\n",
    "ts = str(int(time.time()))\n",
    "\n",
    "train_blend_df = pd.DataFrame(data=dataset_blend_train, index=X.index, columns=['y'])\n",
    "test_blend_df = pd.DataFrame(data=dataset_blend_test, index=test_X.index, columns=['y'])\n",
    "\n",
    "train_blend_df.to_csv(os.path.join(ENSEMBLE_PATH, 'malhot_dnn5fold_{}_{}_{}_train.csv'.format(ts, 0, val_r2_mean)), index=True)\n",
    "test_blend_df.to_csv(os.path.join(ENSEMBLE_PATH, 'malhot_dnn5fold_{}_{}_{}_test.csv'.format(ts, 0, val_r2_mean)), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-7fcfd03ceb0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msubmission_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msubmission_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msubmission_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSUBMISSION_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'submission-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Generated submission '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSUBMISSION_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'submission-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_Y' is not defined"
     ]
    }
   ],
   "source": [
    "submission_df = test_df[['ID']]\n",
    "submission_df['y'] = test_Y.tolist()\n",
    "submission_df.to_csv(os.path.join(SUBMISSION_PATH, 'submission-' + str(int(time.time())) + '.csv'), index=False)\n",
    "print('Generated submission ', os.path.join(SUBMISSION_PATH, 'submission-' + str(int(time.time())) + '.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
